{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Heterogeneous Graph Transformer\n",
    "based on the paper “Heterogeneous Graph Transformer”  by Ziniu Hu, et. al. https://arxiv.org/pdf/2003.01332.pdf\n",
    "The code below is based on \n",
    "https://github.com/dmlc/dgl/tree/master/examples/pytorch/hgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGTLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_types, num_relations, n_heads, dropout = 0.2, use_norm = False):\n",
    "        super(HGTLayer, self).__init__()\n",
    "\n",
    "        self.in_dim        = in_dim\n",
    "        self.out_dim       = out_dim\n",
    "        self.num_types     = num_types\n",
    "        self.num_relations = num_relations\n",
    "        self.n_heads       = n_heads\n",
    "        self.d_k           = out_dim // n_heads\n",
    "        self.sqrt_dk       = math.sqrt(self.d_k)\n",
    "        \n",
    "        self.k_linears   = nn.ModuleList()\n",
    "        self.q_linears   = nn.ModuleList()\n",
    "        self.v_linears   = nn.ModuleList()\n",
    "        self.a_linears   = nn.ModuleList()\n",
    "        self.norms       = nn.ModuleList()\n",
    "        self.use_norm    = use_norm\n",
    "        \n",
    "        for t in range(num_types):\n",
    "            self.k_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.q_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.v_linears.append(nn.Linear(in_dim,   out_dim))\n",
    "            self.a_linears.append(nn.Linear(out_dim,  out_dim))\n",
    "            if use_norm:\n",
    "                self.norms.append(nn.LayerNorm(out_dim))\n",
    "            \n",
    "        self.relation_pri   = nn.Parameter(torch.ones(num_relations, self.n_heads))\n",
    "        self.relation_att   = nn.Parameter(torch.Tensor(num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.relation_msg   = nn.Parameter(torch.Tensor(num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.skip           = nn.Parameter(torch.ones(num_types))\n",
    "        self.drop           = nn.Dropout(dropout)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.relation_att)\n",
    "        nn.init.xavier_uniform_(self.relation_msg)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        etype = edges.data['id'][0]\n",
    "        relation_att = self.relation_att[etype]\n",
    "        relation_pri = self.relation_pri[etype]\n",
    "        relation_msg = self.relation_msg[etype]\n",
    "        key   = torch.bmm(edges.src['k'].transpose(1,0), relation_att).transpose(1,0)\n",
    "        att   = (edges.dst['q'] * key).sum(dim=-1) * relation_pri / self.sqrt_dk\n",
    "        val   = torch.bmm(edges.src['v'].transpose(1,0), relation_msg).transpose(1,0)\n",
    "        return {'a': att, 'v': val}\n",
    "    \n",
    "    def message_func(self, edges):\n",
    "        return {'v': edges.data['v'], 'a': edges.data['a']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        att = F.softmax(nodes.mailbox['a'], dim=1)\n",
    "        h   = torch.sum(att.unsqueeze(dim = -1) * nodes.mailbox['v'], dim=1)\n",
    "        return {'t': h.view(-1, self.out_dim)}\n",
    "        \n",
    "    def forward(self, G, inp_key, out_key):\n",
    "        node_dict, edge_dict = G.node_dict, G.edge_dict\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            k_linear = self.k_linears[node_dict[srctype]]\n",
    "            v_linear = self.v_linears[node_dict[srctype]] \n",
    "            q_linear = self.q_linears[node_dict[dsttype]]\n",
    "            \n",
    "            G.nodes[srctype].data['k'] = k_linear(G.nodes[srctype].data[inp_key]).view(-1, self.n_heads, self.d_k)\n",
    "            G.nodes[srctype].data['v'] = v_linear(G.nodes[srctype].data[inp_key]).view(-1, self.n_heads, self.d_k)\n",
    "            G.nodes[dsttype].data['q'] = q_linear(G.nodes[dsttype].data[inp_key]).view(-1, self.n_heads, self.d_k)\n",
    "            \n",
    "            G.apply_edges(func=self.edge_attention, etype=etype)\n",
    "        G.multi_update_all({etype : (self.message_func, self.reduce_func) \\\n",
    "                            for etype in edge_dict}, cross_reducer = 'mean')\n",
    "        for ntype in G.ntypes:\n",
    "            n_id = node_dict[ntype]\n",
    "            alpha = torch.sigmoid(self.skip[n_id])\n",
    "            trans_out = self.a_linears[n_id](G.nodes[ntype].data['t'])\n",
    "            trans_out = trans_out * alpha + G.nodes[ntype].data[inp_key] * (1-alpha)\n",
    "            if self.use_norm:\n",
    "                G.nodes[ntype].data[out_key] = self.drop(self.norms[n_id](trans_out))\n",
    "            else:\n",
    "                G.nodes[ntype].data[out_key] = self.drop(trans_out)\n",
    "    def __repr__(self):\n",
    "        return '{}(in_dim={}, out_dim={}, num_types={}, num_types={})'.format(\n",
    "            self.__class__.__name__, self.in_dim, self.out_dim,\n",
    "            self.num_types, self.num_relations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGT(nn.Module):\n",
    "    def __init__(self, G, n_inp, n_hid, n_out, n_layers, n_heads, use_norm = True):\n",
    "        super(HGT, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.n_inp = n_inp\n",
    "        self.n_hid = n_hid\n",
    "        self.n_out = n_out\n",
    "        self.n_layers = n_layers\n",
    "        self.adapt_ws  = nn.ModuleList()\n",
    "        for t in range(len(G.node_dict)):\n",
    "            self.adapt_ws.append(nn.Linear(n_inp,   n_hid))\n",
    "        for _ in range(n_layers):\n",
    "            self.gcs.append(HGTLayer(n_hid, n_hid, len(G.node_dict), len(G.edge_dict), n_heads, use_norm = use_norm))\n",
    "        self.out = nn.Linear(n_hid, n_out)\n",
    "\n",
    "    def forward(self, G, out_key):\n",
    "        for ntype in G.ntypes:\n",
    "            n_id = G.node_dict[ntype]\n",
    "            G.nodes[ntype].data['h'] = torch.tanh(self.adapt_ws[n_id](G.nodes[ntype].data['inp']))\n",
    "        for i in range(self.n_layers):\n",
    "            self.gcs[i](G, 'h', 'h')\n",
    "        return self.out(G.nodes[out_key].data['h'])\n",
    "    def __repr__(self):\n",
    "        return '{}(n_inp={}, n_hid={}, n_out={}, n_layers={})'.format(\n",
    "            self.__class__.__name__, self.n_inp, self.n_hid,\n",
    "            self.n_out, self.n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import urllib.request\n",
    "import dgl\n",
    "import math\n",
    "import numpy as np\n",
    "data_url = 'https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/ACM.mat'\n",
    "data_file_path = 'ACM.mat'\n",
    "\n",
    "urllib.request.urlretrieve(data_url, data_file_path)\n",
    "data = scipy.io.loadmat(data_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'author': 17431, 'paper': 12499, 'subject': 73},\n",
      "      num_edges={('paper', 'written-by', 'author'): 37055, ('author', 'writing', 'paper'): 37055, ('paper', 'citing', 'paper'): 30789, ('paper', 'cited', 'paper'): 30789, ('paper', 'is-about', 'subject'): 12499, ('subject', 'has', 'paper'): 12499},\n",
      "      metagraph=[('author', 'paper'), ('paper', 'author'), ('paper', 'paper'), ('paper', 'paper'), ('paper', 'subject'), ('subject', 'paper')])\n"
     ]
    }
   ],
   "source": [
    "G = dgl.heterograph({\n",
    "        ('paper', 'written-by', 'author') : data['PvsA'],\n",
    "        ('author', 'writing', 'paper') : data['PvsA'].transpose(),\n",
    "        ('paper', 'citing', 'paper') : data['PvsP'],\n",
    "        ('paper', 'cited', 'paper') : data['PvsP'].transpose(),\n",
    "        ('paper', 'is-about', 'subject') : data['PvsL'],\n",
    "        ('subject', 'has', 'paper') : data['PvsL'].transpose(),\n",
    "    })\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvc = data['PvsC'].tocsr()\n",
    "# look for papers that appeard in in these conferences\n",
    "#   sosp = 7\n",
    "#   soda = 5\n",
    "#   sigcom =  9\n",
    "#   vldb = 13\n",
    "c_selected = [7, 5, 9, 13] \n",
    "\n",
    "p_selected = pvc[:, c_selected].tocoo()\n",
    "# generate labels\n",
    "labels = pvc.indices\n",
    "labels[labels==0] = 14\n",
    "labels[labels==7] = 0\n",
    "labels[labels==1] = 14\n",
    "labels[labels == 5] = 1\n",
    "labels[labels==2] = 14\n",
    "labels[labels == 9]= 2\n",
    "labels[labels == 3] = 14\n",
    "labels[labels == 13] = 3\n",
    "labels = torch.tensor(labels).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train/val/test split\n",
    "pid = p_selected.row\n",
    "shuffle = np.random.permutation(pid)\n",
    "train_idx = torch.tensor(shuffle[0:1400]).long()\n",
    "val_idx = torch.tensor(shuffle[1400:1500]).long()\n",
    "test_idx = torch.tensor(shuffle[1500:]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 719\n"
     ]
    }
   ],
   "source": [
    "print(len(train_idx), len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.node_dict = {}\n",
    "G.edge_dict = {}\n",
    "for ntype in G.ntypes:\n",
    "    G.node_dict[ntype] = len(G.node_dict)\n",
    "for etype in G.etypes:\n",
    "    G.edge_dict[etype] = len(G.edge_dict)\n",
    "    G.edges[etype].data['id'] = torch.ones(G.number_of_edges(etype), dtype=torch.long) * G.edge_dict[etype] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Random initialize input feature\n",
    "for ntype in G.ntypes:\n",
    "    emb = nn.Parameter(torch.Tensor(G.number_of_nodes(ntype), 400), requires_grad = False)\n",
    "    nn.init.xavier_uniform_(emb)\n",
    "    G.nodes[ntype].data['inp'] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HGT(G, n_inp=400, n_hid=200, n_out=labels.max().item()+1, n_layers=2, n_heads=4, use_norm = True)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, total_steps=200, max_lr = 1e-3, pct_start=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganno\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:  0. LR: 0.00007 Loss 2.5634, Train Acc 0.1129, Val Acc 0.1500 (Best 0.1500), Test Acc 0.1029 (Best 0.1029)\n",
      "ep:  5. LR: 0.00076 Loss 2.1809, Train Acc 0.2593, Val Acc 0.3000 (Best 0.3000), Test Acc 0.2545 (Best 0.2545)\n",
      "ep: 10. LR: 0.00100 Loss 1.3803, Train Acc 0.4079, Val Acc 0.3700 (Best 0.4000), Test Acc 0.3922 (Best 0.3811)\n",
      "ep: 15. LR: 0.00100 Loss 0.8651, Train Acc 0.7336, Val Acc 0.7500 (Best 0.7500), Test Acc 0.7163 (Best 0.7163)\n",
      "ep: 20. LR: 0.00099 Loss 0.4271, Train Acc 0.8850, Val Acc 0.9300 (Best 0.9300), Test Acc 0.8679 (Best 0.8679)\n",
      "ep: 25. LR: 0.00098 Loss 0.3519, Train Acc 0.9121, Val Acc 0.9300 (Best 0.9600), Test Acc 0.9013 (Best 0.8957)\n",
      "ep: 30. LR: 0.00097 Loss 0.2918, Train Acc 0.9236, Val Acc 0.9500 (Best 0.9600), Test Acc 0.9082 (Best 0.8957)\n",
      "ep: 35. LR: 0.00095 Loss 0.2180, Train Acc 0.9357, Val Acc 0.9500 (Best 0.9600), Test Acc 0.9082 (Best 0.8957)\n",
      "ep: 40. LR: 0.00093 Loss 0.1874, Train Acc 0.9450, Val Acc 0.9500 (Best 0.9600), Test Acc 0.9124 (Best 0.8957)\n",
      "ep: 45. LR: 0.00091 Loss 0.1654, Train Acc 0.9514, Val Acc 0.9400 (Best 0.9600), Test Acc 0.9166 (Best 0.8957)\n",
      "ep: 50. LR: 0.00088 Loss 0.1501, Train Acc 0.9521, Val Acc 0.9400 (Best 0.9600), Test Acc 0.9277 (Best 0.8957)\n",
      "ep: 55. LR: 0.00086 Loss 0.1342, Train Acc 0.9621, Val Acc 0.9400 (Best 0.9600), Test Acc 0.9249 (Best 0.8957)\n",
      "ep: 60. LR: 0.00083 Loss 0.1227, Train Acc 0.9650, Val Acc 0.9400 (Best 0.9600), Test Acc 0.9263 (Best 0.8957)\n",
      "ep: 65. LR: 0.00079 Loss 0.1056, Train Acc 0.9686, Val Acc 0.9400 (Best 0.9600), Test Acc 0.9277 (Best 0.8957)\n",
      "ep: 70. LR: 0.00076 Loss 0.0962, Train Acc 0.9743, Val Acc 0.9400 (Best 0.9600), Test Acc 0.9318 (Best 0.8957)\n",
      "ep: 75. LR: 0.00072 Loss 0.0821, Train Acc 0.9807, Val Acc 0.9400 (Best 0.9600), Test Acc 0.9277 (Best 0.8957)\n",
      "ep: 80. LR: 0.00069 Loss 0.0616, Train Acc 0.9864, Val Acc 0.9400 (Best 0.9600), Test Acc 0.9277 (Best 0.8957)\n",
      "ep: 85. LR: 0.00065 Loss 0.0468, Train Acc 0.9921, Val Acc 0.9500 (Best 0.9600), Test Acc 0.9235 (Best 0.8957)\n",
      "ep: 90. LR: 0.00061 Loss 0.0302, Train Acc 0.9950, Val Acc 0.9300 (Best 0.9600), Test Acc 0.9124 (Best 0.8957)\n",
      "ep: 95. LR: 0.00057 Loss 0.0211, Train Acc 0.9957, Val Acc 0.9300 (Best 0.9600), Test Acc 0.9054 (Best 0.8957)\n",
      "ep:100. LR: 0.00052 Loss 0.0148, Train Acc 0.9993, Val Acc 0.9200 (Best 0.9600), Test Acc 0.8971 (Best 0.8957)\n",
      "ep:105. LR: 0.00048 Loss 0.0117, Train Acc 1.0000, Val Acc 0.9400 (Best 0.9600), Test Acc 0.9054 (Best 0.8957)\n",
      "ep:110. LR: 0.00044 Loss 0.0086, Train Acc 1.0000, Val Acc 0.9300 (Best 0.9600), Test Acc 0.9054 (Best 0.8957)\n",
      "ep:115. LR: 0.00040 Loss 0.0073, Train Acc 1.0000, Val Acc 0.9200 (Best 0.9600), Test Acc 0.9026 (Best 0.8957)\n",
      "ep:120. LR: 0.00036 Loss 0.0064, Train Acc 1.0000, Val Acc 0.9200 (Best 0.9600), Test Acc 0.9054 (Best 0.8957)\n",
      "ep:125. LR: 0.00032 Loss 0.0059, Train Acc 1.0000, Val Acc 0.9100 (Best 0.9600), Test Acc 0.9013 (Best 0.8957)\n",
      "ep:130. LR: 0.00028 Loss 0.0054, Train Acc 1.0000, Val Acc 0.9100 (Best 0.9600), Test Acc 0.9110 (Best 0.8957)\n",
      "ep:135. LR: 0.00025 Loss 0.0052, Train Acc 1.0000, Val Acc 0.9200 (Best 0.9600), Test Acc 0.9040 (Best 0.8957)\n",
      "ep:140. LR: 0.00021 Loss 0.0047, Train Acc 1.0000, Val Acc 0.9300 (Best 0.9600), Test Acc 0.9040 (Best 0.8957)\n",
      "ep:145. LR: 0.00018 Loss 0.0045, Train Acc 1.0000, Val Acc 0.9300 (Best 0.9600), Test Acc 0.9054 (Best 0.8957)\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "train_step = 0\n",
    "for epoch in range(150):\n",
    "    logits = model(G, 'paper')\n",
    "    # The loss is computed only for labeled nodes.\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "\n",
    "    pred = logits.argmax(1).cpu()\n",
    "    train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
    "    val_acc   = (pred[val_idx] == labels[val_idx]).float().mean()\n",
    "    test_acc  = (pred[test_idx] == labels[test_idx]).float().mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_step += 1\n",
    "    scheduler.step(train_step)\n",
    "\n",
    "    if best_val_acc < val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = test_acc\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print('ep:%3d. LR: %.5f Loss %.4f, Train Acc %.4f, Val Acc %.4f (Best %.4f), Test Acc %.4f (Best %.4f)' % (epoch,\n",
    "            optimizer.param_groups[0]['lr'], \n",
    "            loss.item(),\n",
    "            train_acc.item(),\n",
    "            val_acc.item(),\n",
    "            best_val_acc,\n",
    "            test_acc.item(),\n",
    "            best_test_acc,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = np.zeros([4,4])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[213.,   0.,   0.,   0.],\n",
       "       [  0., 416.,   0.,   0.],\n",
       "       [  0.,   0., 408.,   0.],\n",
       "       [  0.,   0.,   0., 363.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in train_idx:\n",
    "    if labels[i] == 0:\n",
    "        mat[0, pred[i]]+=1\n",
    "    if labels[i] == 1:\n",
    "        mat[1, pred[i]]+=1\n",
    "    if labels[i] == 2:\n",
    "        mat[2, pred[i]]+=1\n",
    "    if labels[i] == 3:\n",
    "        mat[3, pred[i]]+=1\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 84.,   2.,  17.,   2.],\n",
       "       [  1., 202.,   5.,   3.],\n",
       "       [ 16.,   9., 188.,   5.],\n",
       "       [  0.,   2.,   3., 180.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = np.zeros([4,4])\n",
    "for i in test_idx:\n",
    "    if labels[i] == 0:\n",
    "        mat[0, pred[i]]+=1\n",
    "    if labels[i] == 1:\n",
    "        mat[1, pred[i]]+=1\n",
    "    if labels[i] == 2:\n",
    "        mat[2, pred[i]]+=1\n",
    "    if labels[i] == 3:\n",
    "        mat[3, pred[i]]+=1\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[80.,  2., 16.,  2.],\n",
       "       [ 0., 96.,  2.,  1.],\n",
       "       [ 7.,  4., 86.,  2.],\n",
       "       [ 0.,  1.,  2., 97.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    tot = sum(mat[i,:])\n",
    "    mat[i,:] = np.round(100.0*mat[i,:]/tot,0)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>sosp</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>soda</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sigcom</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>vldb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2     3\n",
       "sosp    80.0   2.0  16.0   2.0\n",
       "soda     0.0  96.0   2.0   1.0\n",
       "sigcom   7.0   4.0  86.0   2.0\n",
       "vldb     0.0   1.0   2.0  97.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(mat, index =['sosp', 'soda', 'sigcom','vldb'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
